{
    "_comment": "This configuration is for training the BERT model with large sequence data. Adjust 'hidden_size' based on available resources. 'data.path' points to the dataset location.",
    "model": {
      "type": "BERT",
      "hidden_size": 768,
      "num_attention_heads": 12,
      "num_hidden_layers": 12,
      "intermediate_size": 3072,
      "hidden_dropout_prob": 0.1
    },
    "data": {
      "_comment": "Path to the input data file. Ensure the file exists at the specified location.",
      "path": "/data/large_sequences.fasta",
      "batch_size": 64,
      "shuffle": true
    },
    "training": {
      "_comment": "Adjust 'learning_rate' based on training behavior. Lower for fine-tuning, higher for training from scratch.",
      "optimizer": {
        "type": "AdamW",
        "learning_rate": 0.00001,
        "weight_decay": 0.01
      },
      "epochs": 10,
      "save_steps": 1000,
      "logging_steps": 100,
      "output_dir": "/models/BERT_pretrained"
    },
    "preprocessing":{
        "strategy":{
            "augmenting": "base"
        
            
        }
    }
  }
  